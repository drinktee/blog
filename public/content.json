{"meta":{"title":"DrinkCode","subtitle":null,"description":null,"author":"drinktee","url":"http://yoursite.com"},"pages":[{"title":"about me","date":"2017-04-14T10:02:30.000Z","updated":"2017-04-14T10:07:27.000Z","comments":true,"path":"about/index.html","permalink":"http://yoursite.com/about/index.html","excerpt":"","text":"Email: zjsxzong89@gmail.com"}],"posts":[{"title":"Kubernetes 1.6 集群部署","slug":"Kubernetes-1-6-集群部署","date":"2017-05-20T10:27:01.000Z","updated":"2017-05-20T12:05:45.000Z","comments":true,"path":"2017/05/20/Kubernetes-1-6-集群部署/","link":"","permalink":"http://yoursite.com/2017/05/20/Kubernetes-1-6-集群部署/","excerpt":"","text":"Kubernetes 1.6 集群部署环境准备 需要先升级内核到3.10以上 下载安装包12wget https://dl.k8s.io/v1.6.2/kubernetes-server-linux-amd64.tar.gztar -zxvf kubernetes-server-linux-amd64.tar.gz 系统配置在各节点创建/etc/sysctl.d/k8s.conf文件，添加如下内容： 12net.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1 执行命令使修改生效 123setenforce 0vi /etc/selinux/configSELINUX=disabled ETCD集群，Docker安装etcd集群采用高可用，三节点部署，启用TLS。在每个node上安装好docker。 Kubernetes各组件TLS证书和密钥生成使用CFSSL来生成证书和密钥。 生成CA证书和私钥ca-config.json文件 123456789101112131415161718&#123; \"signing\": &#123; \"default\": &#123; \"expiry\": \"87600h\" &#125;, \"profiles\": &#123; \"frognew\": &#123; \"usages\": [ \"signing\", \"key encipherment\", \"server auth\", \"client auth\" ], \"expiry\": \"87600h\" &#125; &#125; &#125;&#125; 创建CA证书签名请求配置ca-csr.json： 12345678910111213141516&#123; \"CN\": \"kubernetes\", \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"cloudnative\" &#125; ]&#125; 使用cfssl生成CA证书和私钥 1cfssl gencert -initca ca-csr.json | cfssljson -bare ca 生成kube-apiserver证书和私钥有三个master节点，”11.1.0.1”是api-server的service ipapiserver-csr.json： 12345678910111213141516171819202122232425262728293031&#123; \"CN\": \"kubernetes\", \"hosts\": [ \"127.0.0.1\", \"10.73.212.42\", \"10.73.213.14\", \"10.73.213.31\", \"11.1.0.1\", \"master-mgr00\", \"master-mgr01\", \"master-mgr02\", \"kubernetes\", \"kubernetes.default\", \"kubernetes.default.svc\", \"kubernetes.default.svc.cluster\", \"kubernetes.default.svc.cluster.local\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"k8s\", \"OU\": \"cloudnative\" &#125; ]&#125; 然后生成kube-apiserver的证书和私钥： 123cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=frognew apiserver-csr.json | cfssljson -bare apiserverls apiserver*apiserver.csr apiserver-csr.json apiserver-key.pem apiserver.pem 创建kubernetes-admin客户端证书和私钥123456789101112131415161718192021222324&#123; \"CN\": \"kubernetes-admin\", \"hosts\": [ \"10.73.212.42\", \"10.73.213.14\", \"10.73.213.31\", \"master-mgr00\", \"master-mgr01\", \"master-mgr02\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:masters\", \"OU\": \"cloudnative\" &#125; ]&#125; 生成kubernetes-admin的证书和私钥： 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=frognew admin-csr.json | cfssljson -bare admin 生成kube-controller-manager证书和私钥123456789101112131415161718192021222324&#123; \"CN\": \"system:kube-controller-manager\", \"hosts\": [ \"10.73.212.42\", \"10.73.213.14\", \"10.73.213.31\", \"master-mgr00\", \"master-mgr01\", \"master-mgr02\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:kube-controller-manager\", \"OU\": \"cloudnative\" &#125; ]&#125; 生成证书和私钥 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=frognew controller-manager-csr.json | cfssljson -bare controller-manager 生成kube-scheduler证书和私钥scheduler-csr.json： 123456789101112131415161718192021222324&#123; \"CN\": \"system:kube-scheduler\", \"hosts\": [ \"10.73.212.42\", \"10.73.213.14\", \"10.73.213.31\", \"master-mgr00\", \"master-mgr01\", \"master-mgr02\" ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:kube-scheduler\", \"OU\": \"cloudnative\" &#125; ]&#125; 命令仍然类似 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=frognew scheduler-csr.json | cfssljson -bare scheduler 部署三个master节点三个节点需要先拷贝好各组件的证书与密钥，参考的组件启动参数如下。 注意：这里没有做高可用！ kube-apiserver1234567891011121314151617/home/work/kube_apiserver/bin/kube-apiserver \\--bind-address=0.0.0.0 --insecure-port=8080 \\--secure-port=8443 \\--etcd_servers=https://master-mgr00:2379,\\https://master-mgr01:2379, \\https://master-mgr02:2379 \\--apiserver-count=3 --logtostderr=true --allow-privileged=true \\--tls-cert-file=/home/work/kube_apiserver/conf/apiserver.pem \\--tls-private-key-file=/home/work/kube_apiserver/conf/apiserver-key.pem \\--client-ca-file=/home/work/kube_apiserver/conf/ca.pem \\--service-account-key-file=/home/work/kube_apiserver/conf/ca-key.pem \\--etcd-cafile=/home/work/kube_apiserver/conf/etcd/ca.pem \\--etcd-certfile=/home/work/kube_apiserver/conf/etcd/etcd.pem \\--etcd-keyfile=/home/work/kube_apiserver/conf/etcd/etcd-key.pem \\--admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,\\PersistentVolumeLabel,DefaultStorageClass,ResourceQuota,DefaultTolerationSeconds \\--authorization-mode=RBAC --service-cluster-ip-range=11.1.0.0/16 kube-controller-manager1234567/home/work/kube_controller_manager/bin/kube-controller-manager \\--address=0.0.0.0 --master=http://127.0.0.1:8080 \\--leader-elect=true --logtostderr=true \\ --cluster-cidr=172.17.0.0/16 --allocate-node-cidrs=true \\ --service-cluster-ip-range=11.1.0.0/16 \\ --service-account-private-key-file=/home/work/kube_controller_manager/conf/ca-key.pem \\ --root-ca-file=/home/work/kube_controller_manager/conf/ca.pem kube-scheduler123/home/work/kube_scheduler/bin/kube-scheduler \\--address=0.0.0.0 --leader-elect=true --logtostderr=true \\--master=http://127.0.0.1:8080 Kubernetes Node节点部署 部分机器要先安装下载nsenter这个二进制工具到bin目录 CNI安装12345wget https://github.com/containernetworking/cni/releases/download/v0.5.2/cni-amd64-v0.5.2.tgzmkdir -p /opt/cni/bintar -zxvf cni-amd64-v0.5.2.tgz -C /opt/cni/binls /opt/cni/bin/bridge cnitool dhcp flannel host-local ipvlan loopback macvlan noop ptp tuning kubeletkubelet-csr.json： 123456789101112131415161718&#123; \"CN\": \"system:node\", \"hosts\": [ ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:nodes\", \"OU\": \"cloudnative\" &#125; ]&#125; 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=frognew kubelet-csr.json | cfssljson -bare kubelet 启动参数 12345678910111213/home/work/kubelet/bin/kubelet --address=0.0.0.0 \\--api-servers=https://master-mgr00:8443,https://master-mgr00:8443,https://master-mgr00:8443 \\--pod_infra_container_image=registry.baidu.com/public/pause:2.0 \\--cluster-domain=cluster.local --cluster-dns=11.1.0.10 \\--logtostderr=true \\ --client-ca-file=/home/work/kubelet/conf/ca.pem \\ --tls-private-key-file=/home/work/kubelet/conf/kubelet-key.pem \\ --tls-cert-file=/home/work/kubelet/conf/kubelet.pem \\ --kubeconfig=/home/work/kubelet/conf/kubeconfig.yaml \\ --allow-privileged=true --cgroups-per-qos=false \\ --enforce-node-allocatable= --network-plugin=cni \\ --cni-conf-dir=/etc/cni/net.d --cni-bin-dir=/opt/cni/bin \\ --cadvisor-port=8086 kube-proxykube-proxy-csr.json: 123456789101112131415161718&#123; \"CN\": \"system:kube-proxy\", \"hosts\": [ ], \"key\": &#123; \"algo\": \"rsa\", \"size\": 2048 &#125;, \"names\": [ &#123; \"C\": \"CN\", \"ST\": \"BeiJing\", \"L\": \"BeiJing\", \"O\": \"system:kube-proxy\", \"OU\": \"cloudnative\" &#125; ]&#125; 1cfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=frognew kube-proxy-csr.json | cfssljson -bare kube-proxy 生成kubeconfig： 1234567891011121314151617181920export KUBE_APISERVER=\"https://master-mgr00:8443\"# set-clusterkubectl config set-cluster kubernetes \\ --certificate-authority=/home/work/kube_proxy/conf/ca.pem \\ --embed-certs=true \\ --server=$&#123;KUBE_APISERVER&#125; \\ --kubeconfig=kube-proxy.conf# set-credentialskubectl config set-credentials system:kube-proxy \\ --client-certificate=/home/work/kube_proxy/conf/kube-proxy.pem \\ --embed-certs=true \\ --client-key=/home/work/kube_proxy/conf/kube-proxy-key.pem \\ --kubeconfig=kube-proxy.conf# set-contextkubectl config set-context system:kube-proxy@kubernetes \\ --cluster=kubernetes \\ --user=system:kube-proxy \\ --kubeconfig=kube-proxy.conf# set default contextkubectl config use-context system:kube-proxy@kubernetes --kubeconfig=kube-proxy.conf 启动命令 1234/home/work/kube_proxy/bin/kube-proxy --master=https://master-mgr00:8443 \\--kubeconfig=/home/work/kube_proxy/conf/kube-proxy.conf \\--proxy-mode=iptables --cluster-cidr=172.17.0.0/16 \\--logtostderr=true 总结后续还要部署flannel，dns等其他插件，将写在下一篇博客里了。","categories":[],"tags":[]},{"title":"docker启动容器后，daemon crash问题，An error occurred trying to connect","slug":"docker-run启动daemon-crash问题，An-error-occurred-trying-to-connect","date":"2017-04-16T02:44:54.000Z","updated":"2017-04-16T03:06:10.000Z","comments":true,"path":"2017/04/16/docker-run启动daemon-crash问题，An-error-occurred-trying-to-connect/","link":"","permalink":"http://yoursite.com/2017/04/16/docker-run启动daemon-crash问题，An-error-occurred-trying-to-connect/","excerpt":"","text":"最近运行docker run 命令后出现了，容器无法启动的情况，出现了以下信息。 1234$ docker run -d registry.baidu.com/public/nginx_1-8-0:1.0.133d0ea46c4015f5ec6dee16475b0e66b3077cca6214c6d797c5967bbe2279307An error occurred trying to connect: Post http:///var/run/docker.sock/v1.21/containers/33d0ea46c4015f5ec6dee16475b0e66b3077cca6214c6d797c5967bbe2279307/start: EOF 查看日志后发现panic,docker进程挂了。1234567891011121314151617181920212223242526runtime: goroutine stack exceeds 1000000000-byte limitfatal error: stack overflowruntime stack:runtime.throw(0x1d967b9) /usr/local/go/src/runtime/panic.go:491 +0xadruntime.newstack() /usr/local/go/src/runtime/stack.c:784 +0x555runtime.morestack() /usr/local/go/src/runtime/asm_amd64.s:324 +0x7egoroutine 67 [stack growth]:path/filepath.(*lazybuf).append(0xc228c102a8, 0x2f) /usr/local/go/src/path/filepath/path.go:35 fp=0xc228c101f0 sp=0xc228c101e8path/filepath.Clean(0xc208869e60, 0x1, 0x0, 0x0) /usr/local/go/src/path/filepath/path.go:103 +0x1e4 fp=0xc228c102f8 sp=0xc228c101f0path/filepath.Dir(0xc208869e60, 0x1, 0x0, 0x0) /usr/local/go/src/path/filepath/path.go:454 +0xdd fp=0xc228c10360 sp=0xc228c102f8github.com/opencontainers/runc/libcontainer/cgroups/fs.(*CpusetGroup).ensureParent(0x1dbd8c0, 0xc208869e60, 0x1, 0xc2088b128d, 0x8, 0x0, 0x0) /go/src/github.com/docker/docker/vendor/src/github.com/opencontainers/runc/libcontainer/cgroups/fs/cpuset.go:91 +0x50 fp=0xc228c103e0 sp=0xc228c10360goroutine 10 [chan receive, 1 minutes]:github.com/docker/docker/api/server.(*Server).ServeAPI(0xc208037980, 0x0, 0x0) /go/src/github.com/docker/docker/api/server/server.go:94 +0x1b6main.func·007() /go/src/github.com/docker/docker/docker/daemon.go:255 +0x3bcreated by main.(*DaemonCli).CmdDaemon /go/src/github.com/docker/docker/docker/daemon.go:261 +0x1571 google之后，发现这个ISSUE 有点类似，bschiffthaler将内核升级到3.13.0-70之后，修复了此问题。 @tiborvass Seems it was an issue with the kernel. The error persisted through a reboot, but upgrading the kernel to 3.13.0-70 fixed the issue. We have other machines with kernels older than 3.13.0-34 and docker runs just fine, I would chuck it up to a systems “hiccup”.I’ll be closing the issue with this. 此问题也可能与机器有关，相同内核的不同机器，也会出现此问题。按照ISSUE中所说，在1.10以上版本已经修复此问题。 @sanwan the error discussed here was in docker 1.10 and older, and was resolved; if you’re running the current version of docker, please open a new issue with more details (as requested in the bug report template), or search if there’s an existing open issue.","categories":[],"tags":[]}]}